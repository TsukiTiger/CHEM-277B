{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (a) new (x,y) position: [0.15 0.9 ]\n",
    "\n",
    "    (b) Take 41 steps to converge. Converge to point [-0.99999982  0.99999455] with value -2.9999999999721534.  took: 0.0020 sec \n",
    "\n",
    "3. (b) It is a stochastic method so your answer may vary. It takes ~1700 steps to converge and took ~0.1 sec\n",
    "\n",
    "4. (b) takes ~250 steps to converge and took ~0.02 sec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "A timing decorator. Put at the beginning of your function so that every time your function is called it'll print out the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('func:%r took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that help to visualize the optimization pathway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def draw_path(func,path,x_min=-2,x_max=2,y_min=-2,y_max=2):\n",
    "    a=np.linspace(x_min,x_max,100)\n",
    "    b=np.linspace(y_min,y_max,100)\n",
    "    x,y=np.meshgrid(a,b)\n",
    "    z=func((x,y))\n",
    "    fig,ax=plt.subplots()\n",
    "    my_contour=ax.contour(x,y,z,50)\n",
    "    plt.colorbar(my_contour)\n",
    "    ax.plot(path[:,0],path[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates for algorithm you need to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy.linalg as LA\n",
    "\n",
    "@timeit\n",
    "def steepest_descent(func,first_derivate,starting_point,stepsize,tol):\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count=0\n",
    "    visited=[]\n",
    "    while LA.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        ...\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            ...\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            ...\n",
    "        count+=1\n",
    "    # return the results\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count=0\n",
    "    visited=[]\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            ...\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        ...\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            ...\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            ...\n",
    "        count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDM(func,first_derivate,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection=1):\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count=0\n",
    "    visited=[]\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            ...\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        ...\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            ...\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            # if stepsize is too small, clear previous direction because we already know that is not a useful direction\n",
    "            if stepsize<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                # do the same as SGD here\n",
    "                ...\n",
    "        count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7\n",
    "## Outline\n",
    "\n",
    "* Using Seaborn to visualize statistical data\n",
    "    * Categorical data visualization: catplot\n",
    "    * Continuous data visualization: relplot\n",
    "    * Input feature correlation: pairplot\n",
    "* Correlation matrix\n",
    "* Hierarchical agglomerative clustering\n",
    "* Comparing different unsupervised clustering methods on toy datasets\n",
    "* Q&A on HW7\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Seaborn to visualize statistical data\n",
    "Seaborn is a library for making statistical graphics in Python. It is built on top of matplotlib and closely integrated with pandas data structures.<br>\n",
    "You may need to run <br>\n",
    "**conda install seaborn**\n",
    " <br>\n",
    " if you haven't installed seaborn before\n",
    "[Documentation for Seaborn](http://seaborn.pydata.org/index.html)\n",
    "![1](http://seaborn.pydata.org/_images/scatterplot_matrix.png)\n",
    "\n",
    "### Categorical data visualization: catplot\n",
    "seaborn.catplot()<br>\n",
    "Frequently used arguments:<br>\n",
    "x,y,data,row,col,hue,kind=[“point”, “bar”, “strip”, “swarm”, “box”, “violin”, “boxen”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib widget\n",
    "\n",
    "df=pd.read_csv(\"../../Datasets/titantic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the data first by dropping unrelated features and filter out data with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=...,x=...,y=...,kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=df,x=...,y=...,hue=...,kind=\"violin\",split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous data visualization: relplot\n",
    "seaborn.relplot()<br>\n",
    "Frequently used arguments:<br>\n",
    "x,y,data,row,col,hue,size,style,markers,kind=[\"scatter\",\"line\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2=pd.read_csv(\"../../Datasets/Admission_Predict_Ver1.1.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.relplot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input feature Visualization: pairplot\n",
    "seaborn.pairplot()<br>\n",
    "Frequently used arguments:\n",
    "data,hue,kind=[\"scatter\",\"reg\"],diag_kind=[\"auto\",\"hist\",\"kde\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df3=pd.read_csv(\"../../Datasets/wines.csv\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df3.iloc[:,[5,6,9,10,11,-1]],hue=\"ranking\",diag_kws={'bw': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "The **correlation coefficient** between two random variables $\\mathrm{a}$ and $\\mathrm{b}$ are defined as:\n",
    "$$\\frac{\\mathrm{Cov}(\\mathrm{a},\\mathrm{b})}{\\sqrt{\\mathrm{Var}(\\mathrm{a})\\mathrm{Var}(\\mathrm{b})}}.$$\n",
    "The correlation coefficients are in the range of $[-1,1]$. The positive values represent positive correlations and the negative values represent negative correlations. Basically, it is the \"normalized\" variance that captures the linear relationship of the two random variables. Therefore, if $\\mathrm{x}$ and $\\mathrm{y}$ have relationship $\\mathrm{y}=\\mathrm{x}$, then the correlation coefficient is 1, and if $\\mathrm{y}=-\\mathrm{x}$, then the correlation coefficient is -1.\n",
    "\n",
    "<br>\n",
    "The correlation matrix $\\boldsymbol{R}\\in\\mathbb{R}^{D\\times D}$ of a random vector $\\boldsymbol{x}$ is a square matrix whose each element $R_{ij}$\n",
    "denotes the correlation between the attributes $\\mathrm{x_i}$\n",
    "and $\\mathrm{x_j}$\n",
    ". If we regard data points as the i.i.d. samples of $\\boldsymbol{x}$\n",
    ", then we can have an estimate $\\hat{\\boldsymbol{R}}$\n",
    "whose each element\n",
    "$$\\hat{R}_{i,j}=\\frac{\\Sigma_{s=1}^{N}(x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}})(x_{j}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{j}})}{\\sqrt{\\Sigma_{s=1}^{N}(x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}})^{2}}\\sqrt{\\Sigma_{s=1}^{N}(x_{j}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{j}})^{2}}}=\\frac{\\hat{\\sigma}_{\\mathrm{x}_{i},\\mathrm{x}_{j}}}{\\hat{\\sigma}_{\\mathrm{x}_{i}}\\hat{\\sigma}_{\\mathrm{x}_{j}}}$$\n",
    "is an estimate of the correlation (usually called the **Pearson's R**) between attribute $\\mathrm{x_i}$\n",
    "and $\\mathrm{x_j}$\n",
    ". Note that if we **z-normalize** each data point such that\n",
    "$$z_{i}^{(s)}=\\frac{x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}}}{\\hat{\\sigma}_{\\mathrm{x}_{i}}}$$\n",
    "for all $i$. Then we simply have $\\hat{\\boldsymbol{R}}=\\frac{1}{N}\\boldsymbol{Z}^\\top \\boldsymbol{Z}$, where $\\boldsymbol{Z}$ is the design matrix of the normalized data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X=df3.drop([\"Start assignment\",'ranking'],axis=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "Z=sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "P=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a heatmap to visualize the correlation: [sns heatmap documentation](http://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hm=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: we could have simply used the NumPy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "P = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the estimate $\\hat{\\boldsymbol{P}}$\n",
    "of the correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x=np.random.random(1000)*10\n",
    "#add gaussian distributed noise\n",
    "y=x+np.random.normal(0,3,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x,y,s=0.5)\n",
    "plt.plot([0,10],[0,10],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical agglomerative clustering\n",
    "\n",
    "## Dataset\n",
    "We will work with the Iris dataset as our toy example, and for simplicity and visualization we just keep 2-dimensional features, i.e. sepal length and petal width. We want our features to be normalized to range [0,1]. The classification of the dataset looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../Datasets/iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all features to a number in range(0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal_length\"] /= df[\"sepal_length\"].max()\n",
    "df[\"sepal_width\"] /= df[\"sepal_width\"].max()\n",
    "df[\"petal_length\"] /= df[\"petal_length\"].max()\n",
    "df[\"petal_width\"] /= df[\"petal_width\"].max()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df, x=\"petal_length\", y=\"petal_width\", hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrograms and cutting dendrograms to form clusters\n",
    "Hierarchical clustering can be either bottom-up (agglomerative) or top-down (divisive), but the former one is used much more frequently in real-world applications. The nice property of hierarchical clustering is that it creates a information-rich dendrogram for understanding the relative distance of any two data points in the group.\n",
    "\n",
    "Here I show how we can accomplish hierarchical clustering with scipy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sc\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(20, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "\n",
    "Z=...\n",
    "# Create dendrogram\n",
    "...\n",
    "\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Euclidean distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the trucated dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('Euclidean distance')\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to cut the denrogram and form clusters.<br>\n",
    "Use distance as criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "print(clusters)\n",
    "ax=sns.relplot(data=df, x=\"sepal_length\", y=\"petal_width\", hue=clusters, legend='full')\n",
    "ax.fig.suptitle(\"HAC distance cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "print(clusters)\n",
    "hues = [chr(c+96) for c in clusters]\n",
    "ax2=sns.relplot(data=df, x=\"sepal_length\", y=\"petal_width\", hue=hues, legend='full')\n",
    "ax2.fig.suptitle(\"HAC ncluster=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on `scipy.cluster.hierarchy.linkage`: [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n",
    "<br><br>\n",
    "## Fitting an agglomerative clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = ...\n",
    "...\n",
    "\n",
    "\n",
    "ax1=sns.relplot(data=df, x=\"sepal_length\", y=\"petal_width\", hue=\"class\")\n",
    "ax1.fig.suptitle(\"True classification\")\n",
    "\n",
    "ax2=sns.relplot(data=df, x=\"sepal_length\", y=\"petal_width\", hue=labels, legend=False)\n",
    "ax2.fig.suptitle(\"Agglomerative clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different upsupervised clustering methods on toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 5,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"xi\": 0.25,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"xi\": 0.035,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.KMeans(n_clusters=params[\"n_clusters\"])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "    )\n",
    "\n",
    "    dbscan = cluster.DBSCAN(eps=params[\"eps\"],min_samples=params[\"min_samples\"])\n",
    "\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        affinity=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "#         connectivity=connectivity,\n",
    "    )\n",
    "\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"], covariance_type=\"full\"\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"KMeans\", two_means),\n",
    "        (\"HAC \\nWard linkage\", ward),\n",
    "        (\"HAC \\naverage linkage\", average_linkage),\n",
    "        (\"DBSCAN\", dbscan),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                + \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
